{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable, gradcheck\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "import mne\n",
    "from mne.io import concatenate_raws, read_raw_fif\n",
    "import mne.viz\n",
    "\n",
    "import math\n",
    "\n",
    "from os import walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-0.698</th>\n",
       "      <th>-0.762</th>\n",
       "      <th>-0.944</th>\n",
       "      <th>-1.122</th>\n",
       "      <th>-1.108</th>\n",
       "      <th>-0.723</th>\n",
       "      <th>0.07</th>\n",
       "      <th>1.125</th>\n",
       "      <th>2.155</th>\n",
       "      <th>2.898</th>\n",
       "      <th>...</th>\n",
       "      <th>20.833</th>\n",
       "      <th>20.093</th>\n",
       "      <th>19.654</th>\n",
       "      <th>19.536</th>\n",
       "      <th>19.697</th>\n",
       "      <th>20.011</th>\n",
       "      <th>20.335</th>\n",
       "      <th>20.588</th>\n",
       "      <th>20.827</th>\n",
       "      <th>21.199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>-3.356</td>\n",
       "      <td>-3.230</td>\n",
       "      <td>-2.968</td>\n",
       "      <td>-2.780</td>\n",
       "      <td>-2.813</td>\n",
       "      <td>-3.067</td>\n",
       "      <td>-3.422</td>\n",
       "      <td>-3.729</td>\n",
       "      <td>-3.881</td>\n",
       "      <td>-3.842</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.171</td>\n",
       "      <td>-5.669</td>\n",
       "      <td>-6.558</td>\n",
       "      <td>-7.532</td>\n",
       "      <td>-8.214</td>\n",
       "      <td>-8.319</td>\n",
       "      <td>-7.755</td>\n",
       "      <td>-6.610</td>\n",
       "      <td>-5.066</td>\n",
       "      <td>-3.331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>-0.936</td>\n",
       "      <td>-1.833</td>\n",
       "      <td>-2.833</td>\n",
       "      <td>-3.619</td>\n",
       "      <td>-3.919</td>\n",
       "      <td>-3.637</td>\n",
       "      <td>-2.916</td>\n",
       "      <td>-2.085</td>\n",
       "      <td>-1.501</td>\n",
       "      <td>-1.373</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.416</td>\n",
       "      <td>-2.966</td>\n",
       "      <td>-2.921</td>\n",
       "      <td>-3.143</td>\n",
       "      <td>-3.330</td>\n",
       "      <td>-3.237</td>\n",
       "      <td>-2.777</td>\n",
       "      <td>-1.950</td>\n",
       "      <td>-0.725</td>\n",
       "      <td>0.973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1.232</td>\n",
       "      <td>0.121</td>\n",
       "      <td>-0.805</td>\n",
       "      <td>-1.356</td>\n",
       "      <td>-1.463</td>\n",
       "      <td>-1.185</td>\n",
       "      <td>-0.671</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.447</td>\n",
       "      <td>0.963</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.272</td>\n",
       "      <td>-2.243</td>\n",
       "      <td>-3.477</td>\n",
       "      <td>-4.764</td>\n",
       "      <td>-5.790</td>\n",
       "      <td>-6.248</td>\n",
       "      <td>-5.933</td>\n",
       "      <td>-4.799</td>\n",
       "      <td>-2.954</td>\n",
       "      <td>-0.630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.585</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.452</td>\n",
       "      <td>0.721</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.278</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>-0.810</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.846</td>\n",
       "      <td>-7.245</td>\n",
       "      <td>-6.739</td>\n",
       "      <td>-6.557</td>\n",
       "      <td>-6.794</td>\n",
       "      <td>-7.355</td>\n",
       "      <td>-8.004</td>\n",
       "      <td>-8.513</td>\n",
       "      <td>-8.786</td>\n",
       "      <td>-8.876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>4.899</td>\n",
       "      <td>5.001</td>\n",
       "      <td>4.836</td>\n",
       "      <td>4.464</td>\n",
       "      <td>3.933</td>\n",
       "      <td>3.295</td>\n",
       "      <td>2.640</td>\n",
       "      <td>2.102</td>\n",
       "      <td>1.814</td>\n",
       "      <td>1.850</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.848</td>\n",
       "      <td>-2.510</td>\n",
       "      <td>-2.391</td>\n",
       "      <td>-2.514</td>\n",
       "      <td>-2.775</td>\n",
       "      <td>-2.982</td>\n",
       "      <td>-2.920</td>\n",
       "      <td>-2.431</td>\n",
       "      <td>-1.453</td>\n",
       "      <td>-0.030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 384 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    -0.698  -0.762  -0.944  -1.122  -1.108  -0.723   0.07  1.125  2.155  \\\n",
       "58  -3.356  -3.230  -2.968  -2.780  -2.813  -3.067 -3.422 -3.729 -3.881   \n",
       "59  -0.936  -1.833  -2.833  -3.619  -3.919  -3.637 -2.916 -2.085 -1.501   \n",
       "60   1.232   0.121  -0.805  -1.356  -1.463  -1.185 -0.671 -0.093  0.447   \n",
       "61   0.585   0.034  -0.144   0.065   0.452   0.721  0.667  0.278 -0.293   \n",
       "62   4.899   5.001   4.836   4.464   3.933   3.295  2.640  2.102  1.814   \n",
       "\n",
       "    2.898  ...  20.833  20.093  19.654  19.536  19.697  20.011  20.335  \\\n",
       "58 -3.842  ...  -5.171  -5.669  -6.558  -7.532  -8.214  -8.319  -7.755   \n",
       "59 -1.373  ...  -3.416  -2.966  -2.921  -3.143  -3.330  -3.237  -2.777   \n",
       "60  0.963  ...  -1.272  -2.243  -3.477  -4.764  -5.790  -6.248  -5.933   \n",
       "61 -0.810  ...  -7.846  -7.245  -6.739  -6.557  -6.794  -7.355  -8.004   \n",
       "62  1.850  ...  -2.848  -2.510  -2.391  -2.514  -2.775  -2.982  -2.920   \n",
       "\n",
       "    20.588  20.827  21.199  \n",
       "58  -6.610  -5.066  -3.331  \n",
       "59  -1.950  -0.725   0.973  \n",
       "60  -4.799  -2.954  -0.630  \n",
       "61  -8.513  -8.786  -8.876  \n",
       "62  -2.431  -1.453  -0.030  \n",
       "\n",
       "[5 rows x 384 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at some of the data\n",
    "data_file = 'study1_eeg/study1_EEG_P-01_FN_Trial-001.csv'\n",
    "\n",
    "data_P_09 = pd.read_csv(data_file)\n",
    "data_P_09.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<EpochsFIF  |   16666 events (all good), 0 - 1.49609 sec, baseline off, ~3.05 GB, data loaded,\n",
      " 'FN': 4336\n",
      " 'FP': 4371\n",
      " 'FU': 4239\n",
      " 'NN': 1238\n",
      " 'NP': 1264\n",
      " 'NU': 1218>\n"
     ]
    }
   ],
   "source": [
    "# take some data that was already formatted, from this link: https://neuro.inf.unibe.ch/AlgorithmsNeuroscience/Tutorial_files/DatasetConstruction.html\n",
    "data_file = 'study1_eeg/epochdata/master'\n",
    "\n",
    "# Read the EEG epochs:\n",
    "epochs = mne.read_epochs(data_file + '.fif', verbose='error')\n",
    "print(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<EpochsFIF  |   8575 events (all good), 0 - 1.49609 sec, baseline off, ~1.57 GB, data loaded,\n",
      " 'FN': 4336\n",
      " 'FU': 4239>\n",
      "8575\n"
     ]
    }
   ],
   "source": [
    "epochs_UN = epochs['FU', 'FN'] # Unpleasant vs. Neutral\n",
    "epochs_UP = epochs['FU', 'FP'] # Unpleasant vs. Pleasant\n",
    "epochs_NP = epochs['FN', 'FP'] # Neutral vs. Pleasant\n",
    "\n",
    "# Dataset with unpleasant and neutral events\n",
    "print(epochs_UN)\n",
    "data_UN = epochs_UN.get_data() #we will classify between unpleasant and neutral\n",
    "labels_UN = epochs_UN.events[:,-1]\n",
    "print(len(labels_UN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_UN, test_data_UN, labels_train_UN, labels_test_UN = train_test_split(data_UN, labels_UN, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6002,) (2573,)\n"
     ]
    }
   ],
   "source": [
    "print(labels_train_UN.shape, labels_test_UN.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6002, 1, 64, 384]) torch.Size([6002, 1]) torch.Size([2573, 1, 64, 384])\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 301\n",
    "\n",
    "eeg_data_scaler = StandardScaler()\n",
    "\n",
    "X_train = eeg_data_scaler.fit_transform(train_data_UN.reshape(-1, train_data_UN.shape[-1])).reshape(train_data_UN.shape)\n",
    "X_test = eeg_data_scaler.fit_transform(test_data_UN.reshape(-1, test_data_UN.shape[-1])).reshape(test_data_UN.shape)\n",
    "\n",
    "labels_train_UN = np.array([1 if x > 0 else 0 for x in labels_train_UN])\n",
    "labels_test_UN = np.array([1 if x > 0 else 0 for x in labels_test_UN])\n",
    "\n",
    "labels_train_UN = labels_train_UN.reshape((labels_train_UN.shape[0], 1))\n",
    "labels_train_UN = labels_train_UN.astype(np.float32)\n",
    "X_actual = torch.from_numpy(labels_train_UN)\n",
    "\n",
    "labels_test_UN = labels_test_UN.reshape((labels_test_UN.shape[0], 1))\n",
    "labels_test_UN = labels_test_UN.astype(np.float32)\n",
    "X_test_actual = torch.from_numpy(labels_test_UN)\n",
    "\n",
    "X_train = torch.from_numpy(X_train)\n",
    "X_train = X_train.unsqueeze(1)\n",
    "X_test = torch.from_numpy(X_test)\n",
    "X_test = X_test.unsqueeze(1)\n",
    "\n",
    "X_list = [0] * (math.ceil(X_train.shape[0] / BATCH_SIZE))\n",
    "for i in range(len(X_list)):\n",
    "    a, b = BATCH_SIZE * i, BATCH_SIZE * (i + 1)\n",
    "    if i != len(X_list) - 1:\n",
    "        X_list[i] = (X_train[a:b, :, : ], X_actual[a:b, :])\n",
    "    else:\n",
    "        X_list[i] = (X_train[a:, :, : ], X_actual[a:, :])\n",
    "\n",
    "\n",
    "print(X_train.shape, X_actual.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1_neurons = 4\n",
    "conv2_neurons = 8\n",
    "conv3_neurons = 4\n",
    "flat1_in = 168\n",
    "flat1_out = 64\n",
    "flat2_out = 256\n",
    "flat3_out = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNNPoor = nn.Sequential(\n",
    "    nn.Conv2d(1, conv1_neurons, (1, 128), padding=1),\n",
    "    nn.ELU(),\n",
    "    nn.BatchNorm2d(conv1_neurons, False),\n",
    "    \n",
    "    nn.Conv2d(conv1_neurons, conv2_neurons, (64, 1), padding=1),\n",
    "    nn.ELU(),\n",
    "    nn.BatchNorm2d(conv2_neurons, False),\n",
    "    nn.AvgPool2d((1, 4)),\n",
    "    nn.Dropout(),\n",
    "    \n",
    "    nn.Conv2d(conv2_neurons, conv3_neurons, (1, 16), padding=1),\n",
    "    nn.ELU(),\n",
    "    nn.BatchNorm2d(conv3_neurons, False),\n",
    "    nn.AvgPool2d((1, 8)),\n",
    "    nn.Dropout(),\n",
    "    \n",
    "    nn.Flatten(),\n",
    "\n",
    "    nn.Linear(flat1_in, flat1_out),\n",
    "    nn.ELU(),\n",
    "    nn.Linear(flat1_out, 1),\n",
    "    nn.Sigmoid(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.Adam(CNNPoor.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        content, labels = data\n",
    "        pred = model(content)\n",
    "        pred = pred.numpy()\n",
    "    return accuracy_score(labels, np.round(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Total loss =  12.569632589817047\n",
      "Train accuracy =  0.653615461512829\n",
      "Test accuracy =  0.5787019043917606\n",
      "Epoch:  1\n",
      "Total loss =  12.575608730316162\n",
      "Train accuracy =  0.64595134955015\n",
      "Test accuracy =  0.5616012436844151\n",
      "Epoch:  2\n",
      "Total loss =  12.529138326644897\n",
      "Train accuracy =  0.6607797400866378\n",
      "Test accuracy =  0.5744267392149243\n",
      "Epoch:  3\n",
      "Total loss =  12.556964874267578\n",
      "Train accuracy =  0.6514495168277241\n",
      "Test accuracy =  0.5779246016323358\n",
      "Epoch:  4\n",
      "Total loss =  12.50970470905304\n",
      "Train accuracy =  0.6584471842719094\n",
      "Test accuracy =  0.5806451612903226\n",
      "Epoch:  5\n",
      "Total loss =  12.43011748790741\n",
      "Train accuracy =  0.6591136287904032\n",
      "Test accuracy =  0.586863583365721\n",
      "Epoch:  6\n",
      "Total loss =  12.418574512004852\n",
      "Train accuracy =  0.6607797400866378\n",
      "Test accuracy =  0.567431014380101\n",
      "Epoch:  7\n",
      "Total loss =  12.402537107467651\n",
      "Train accuracy =  0.6649450183272243\n",
      "Test accuracy =  0.5643218033424019\n",
      "Epoch:  8\n",
      "Total loss =  12.242958128452301\n",
      "Train accuracy =  0.6644451849383539\n",
      "Test accuracy =  0.5724834823163623\n",
      "Epoch:  9\n",
      "Total loss =  12.226454973220825\n",
      "Train accuracy =  0.6742752415861379\n",
      "Test accuracy =  0.5767586474931986\n",
      "Epoch:  10\n",
      "Total loss =  12.085898280143738\n",
      "Train accuracy =  0.6701099633455515\n",
      "Test accuracy =  0.5841430237077342\n",
      "Epoch:  11\n",
      "Total loss =  12.133612930774689\n",
      "Train accuracy =  0.6797734088637121\n",
      "Test accuracy =  0.5919160513019821\n",
      "Epoch:  12\n",
      "Total loss =  12.263167023658752\n",
      "Train accuracy =  0.6692769076974342\n",
      "Test accuracy =  0.5919160513019821\n",
      "Epoch:  13\n",
      "Total loss =  12.157616317272186\n",
      "Train accuracy =  0.6717760746417861\n",
      "Test accuracy =  0.5759813447337738\n",
      "Epoch:  14\n",
      "Total loss =  12.069766283035278\n",
      "Train accuracy =  0.6797734088637121\n",
      "Test accuracy =  0.5689856198989507\n",
      "Epoch:  15\n",
      "Total loss =  12.03337448835373\n",
      "Train accuracy =  0.6684438520493169\n",
      "Test accuracy =  0.5825884181888845\n",
      "Epoch:  16\n",
      "Total loss =  11.971116662025452\n",
      "Train accuracy =  0.6776074641786072\n",
      "Test accuracy =  0.5860862806062962\n",
      "Epoch:  17\n",
      "Total loss =  12.009714186191559\n",
      "Train accuracy =  0.6802732422525825\n",
      "Test accuracy =  0.5864749319860085\n",
      "Epoch:  18\n",
      "Total loss =  11.961505115032196\n",
      "Train accuracy =  0.6797734088637121\n",
      "Test accuracy =  0.5821997668091722\n",
      "Epoch:  19\n",
      "Total loss =  11.876688778400421\n",
      "Train accuracy =  0.6759413528823726\n",
      "Test accuracy =  0.5825884181888845\n",
      "Epoch:  20\n",
      "Total loss =  11.92936646938324\n",
      "Train accuracy =  0.6867710763078974\n",
      "Test accuracy =  0.5891954916439953\n",
      "Epoch:  21\n",
      "Total loss =  11.833063781261444\n",
      "Train accuracy =  0.6831056314561813\n",
      "Test accuracy =  0.5837543723280217\n",
      "Epoch:  22\n",
      "Total loss =  11.813645005226135\n",
      "Train accuracy =  0.6894368543818727\n",
      "Test accuracy =  0.588806840264283\n",
      "Epoch:  23\n",
      "Total loss =  11.644734859466553\n",
      "Train accuracy =  0.6904365211596135\n",
      "Test accuracy =  0.573260785075787\n",
      "Epoch:  24\n",
      "Total loss =  11.634004473686218\n",
      "Train accuracy =  0.688103965344885\n",
      "Test accuracy =  0.582977069568597\n"
     ]
    }
   ],
   "source": [
    "for i in range(25):\n",
    "    print(\"Epoch: \", i)\n",
    "    tot_loss = 0.0\n",
    "\n",
    "    for j in range(math.ceil(X_train.shape[0] / BATCH_SIZE)):\n",
    "        data, labels = X_list[j]\n",
    "        data, labels = Variable(data.float()), Variable(labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        classification = CNNPoor(data)\n",
    "        loss = loss_function(classification, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        tot_loss += loss.item()\n",
    "    print(\"Total loss = \", tot_loss)\n",
    "    print(\"Train accuracy = \", evaluate(CNNPoor, (X_train.float(), X_actual)))\n",
    "    print(\"Test accuracy = \", evaluate(CNNPoor, (X_test.float(), X_test_actual)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
