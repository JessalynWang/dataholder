{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EEGNet_current_full_copy_labeled.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D415xY3P1PCN",
        "colab_type": "text"
      },
      "source": [
        "# Loose EEGNet\n",
        "This notebook is a neural network that is based as much off of the EEGNet paper as I could understand."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sdi7BvnG2eiJ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VIVsTD-24Dj",
        "colab_type": "text"
      },
      "source": [
        "# Import/Install all necessary packages and libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKZ_PXcuvZIx",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "1cf75d4c-7f21-4e09-9371-f381e412e834"
      },
      "source": [
        "!pip install mne"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mne in /usr/local/lib/python3.6/dist-packages (0.20.8)\n",
            "Requirement already satisfied: scipy>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from mne) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from mne) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anVJZVW0sUwY",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, precision_score, recall_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable, gradcheck\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from matplotlib import pyplot\n",
        "\n",
        "import mne\n",
        "from mne.io import concatenate_raws, read_raw_fif\n",
        "import mne.viz\n",
        "\n",
        "import math\n",
        "\n",
        "from os import walk"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4ou4gyC4dQu",
        "colab_type": "text"
      },
      "source": [
        "# Check for GPU availability and set device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_rTCb13APF7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6t-hO2E4mHe",
        "colab_type": "text"
      },
      "source": [
        "# Load and format the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PrZ39Hzvbcu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "1d3942e7-36a4-4f7e-b3ea-48ccf56a9798"
      },
      "source": [
        "data_file = '/content/drive/My Drive/data/P_01.fif'\n",
        "\n",
        "epochs = mne.read_epochs(data_file, verbose='error')\n",
        "print(epochs.info)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<Info | 10 non-empty values\n",
            " bads: []\n",
            " ch_names: Fp1, AF7, AF3, F1, F3, F5, F7, FT7, FC5, FC3, FC1, C1, C3, C5, ...\n",
            " chs: 64 EEG\n",
            " custom_ref_applied: False\n",
            " dig: 67 items (3 Cardinal, 64 EEG)\n",
            " file_id: 4 items (dict)\n",
            " highpass: 0.0 Hz\n",
            " lowpass: 128.0 Hz\n",
            " meas_date: unspecified\n",
            " meas_id: 4 items (dict)\n",
            " nchan: 64\n",
            " projs: []\n",
            " sfreq: 256.0 Hz\n",
            ">\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FDYPc5GvwTv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "f0852f91-df96-4d7d-a821-9061c39ec289"
      },
      "source": [
        "epochs_UN = epochs['FU', 'FN'] # Unpleasant vs. Neutral\n",
        "epochs_UP = epochs['FU', 'FP'] # Unpleasant vs. Pleasant\n",
        "epochs_NP = epochs['FN', 'FP'] # Neutral vs. Pleasant\n",
        "\n",
        "# Dataset with unpleasant and neutral events\n",
        "print(epochs_UN)\n",
        "data_UN = epochs_UN.get_data() #we will classify between unpleasant and neutral\n",
        "labels_UN = epochs_UN.events[:,-1]\n",
        "print(len(labels_UN))"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<EpochsFIF  |   249 events (all good), 0 - 1.49609 sec, baseline off, ~46.9 MB, data loaded,\n",
            " 'FN': 126\n",
            " 'FU': 123>\n",
            "249\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PzcJwNMv4QU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.manual_seed(3)\n",
        "torch.cuda.manual_seed(3)\n",
        "train_data_UN, test_data_UN, labels_train_UN, labels_test_UN = train_test_split(data_UN, labels_UN, test_size=0.3, random_state=42)"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnIbNDuBv917",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "49927d15-5adc-4f82-a533-be762bf0f20b"
      },
      "source": [
        "print(labels_train_UN.shape, labels_test_UN.shape, train_data_UN.shape[-1])\n",
        "chunk_train = labels_train_UN.shape[0]\n",
        "chunk_test = labels_test_UN.shape[0]\n",
        "channels = train_data_UN.shape[1]\n",
        "timepoints = train_data_UN.shape[2]\n"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(174,) (75,) 384\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f6NOpVZwDwQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "91bb4673-14e1-4741-c370-59ff47404024"
      },
      "source": [
        "BATCH_SIZE = 8\n",
        "BATCH_SIZE2 = chunk_test\n",
        "\n",
        "eeg_data_scaler = StandardScaler()\n",
        "\n",
        "X_train = eeg_data_scaler.fit_transform(train_data_UN.reshape(-1, train_data_UN.shape[-1])).reshape(train_data_UN.shape)\n",
        "X_test = eeg_data_scaler.fit_transform(test_data_UN.reshape(-1, test_data_UN.shape[-1])).reshape(test_data_UN.shape)\n",
        "\n",
        "labels_train_UN = np.array([1 if x > 0 else 0 for x in labels_train_UN])\n",
        "labels_test_UN = np.array([1 if x > 0 else 0 for x in labels_test_UN])\n",
        "\n",
        "labels_train_UN = labels_train_UN.reshape((chunk_train, 1))\n",
        "labels_train_UN = labels_train_UN.astype(np.float32)\n",
        "X_actual = torch.from_numpy(labels_train_UN)\n",
        "\n",
        "labels_test_UN = labels_test_UN.reshape((chunk_test, 1))\n",
        "labels_test_UN = labels_test_UN.astype(np.float32)\n",
        "X_test_actual = torch.from_numpy(labels_test_UN)\n",
        "\n",
        "X_train = torch.from_numpy(X_train)\n",
        "X_train = X_train.unsqueeze(1)\n",
        "X_test = torch.from_numpy(X_test)\n",
        "X_test = X_test.unsqueeze(1)\n",
        "\n",
        "train_batches = math.ceil(chunk_train / BATCH_SIZE)\n",
        "test_batches = math.ceil(chunk_test / BATCH_SIZE2)\n",
        "print(X_train.shape, X_actual.shape, X_test.shape, train_batches, test_batches)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([174, 1, 64, 384]) torch.Size([174, 1]) torch.Size([75, 1, 64, 384]) 22 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vz3MXsZebdIE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set = TensorDataset(X_train, X_actual)\n",
        "test_set = TensorDataset(X_test, X_test_actual)\n",
        "\n",
        "train_set_loader = DataLoader(train_set, batch_size = BATCH_SIZE, shuffle=False)\n",
        "test_set_loader = DataLoader(test_set, batch_size = BATCH_SIZE2, shuffle=False)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCqJTTUu4xuf",
        "colab_type": "text"
      },
      "source": [
        "# Build the model and train\n",
        "A short breakdown of the paper:\n",
        "> EEGNet is a CNN for classification and interpretation of EEG-based BCI's\n",
        "\n",
        "> Benefits of EEGNet:\n",
        "> 1. Can be applied to different BCI paradagims (MI, ERP, SSVEP)\n",
        "> 2. Can be trained with very limited data\n",
        "> 3. Can produce neurophysiologically interpretable features\n",
        "\n",
        "> The model architecture:\n",
        "> 1. Fit 2D convolutional filters of size (1, sampling rate//2)\n",
        "> 2. Use a depthwise convolution of size (# of channels, 1)\n",
        "> 3. Add a separable convolution of size (1, 16)\n",
        "> 4. Flatten the data and feed it through a classification layer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQpyCFRxx24X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hyperparameters\n",
        "freq, avg1stride, avg2stride = 256, (1, 4), (1, 8)\n",
        "convstride = 1 # stride for each conv2D\n",
        "conv1_neurons = 4\n",
        "conv2_neurons = 8\n",
        "conv3_neurons = 16\n",
        "conv4_neurons = 1\n",
        "kern1size = freq // 2\n",
        "kern3size = 32"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpvKD47DQ8FS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "padding_needed = (kern1size - 1) / 2\n",
        "conv1outx, conv1outy = (channels, (timepoints + (2 * padding_needed) - kern1size)/convstride + 1)\n",
        "\n",
        "conv2outx, conv2outy = ((conv1outx - channels)/convstride + 1, conv1outy)\n",
        "conv2outx, conv2outy = conv2outx // avg1stride[0], conv2outy // avg1stride[1]\n",
        "\n",
        "conv3outx, conv3outy = (conv2outx, (conv2outy - kern3size)/convstride + 1)\n",
        "\n",
        "conv4outx, conv4outy = (conv3outx, conv3outy)\n",
        "conv4outx, conv4outy = (conv4outx // avg2stride[0], conv4outy // avg2stride[1])\n",
        "flat1_in = int(conv4outx * conv4outy * conv4_neurons)"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgHA3ZVMfyhn",
        "colab_type": "text"
      },
      "source": [
        "Model description, layer by layer\n",
        "\n",
        "1. Temporal convolution\n",
        "> * - Uses 4 filters of (1, sampling frequency // 2)\n",
        "> * - Learns frequency filters at 2Hz and above\n",
        "2. Depthwise convolution\n",
        "> * - Learns spatial filters\n",
        "3. Separable Convolution\n",
        "> * - Conists of a deptwise convolution followed by a pointwise convolution\n",
        "> * - First learnes a kernel summarizing each feature map, then merges the outputs\n",
        "4. Fully connected layer\n",
        "> * - Consists of a linear layer that reduces the channels, followed by a sigmoid classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCNgBinI_3Oq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "45d9d9a1-2b1e-443b-da5d-5ad673331afb"
      },
      "source": [
        "CNNPoor = nn.Sequential(\n",
        "    nn.ZeroPad2d((math.floor(padding_needed), math.ceil(padding_needed), 0, 0)),\n",
        "    nn.Conv2d(1, conv1_neurons, (1, kern1size), bias=False),\n",
        "    nn.ELU(),\n",
        "    nn.BatchNorm2d(conv1_neurons),\n",
        "    \n",
        "    nn.Conv2d(conv1_neurons, conv2_neurons, (channels, 1), bias=False, groups = conv1_neurons),\n",
        "    nn.ELU(),\n",
        "    nn.BatchNorm2d(conv2_neurons),\n",
        "    nn.AvgPool2d(avg1stride),\n",
        "    nn.Dropout(),\n",
        "    \n",
        "    nn.Conv2d(conv2_neurons, conv3_neurons, (1, kern3size), bias=False, groups = conv2_neurons),\n",
        "    nn.Conv2d(conv3_neurons, conv4_neurons, kernel_size=1, bias=False),\n",
        "    nn.ELU(),\n",
        "    nn.BatchNorm2d(conv4_neurons),\n",
        "    nn.AvgPool2d(avg2stride),\n",
        "    nn.Dropout(),\n",
        "    \n",
        "    nn.Flatten(),\n",
        "\n",
        "    nn.Linear(flat1_in, 1),\n",
        "    nn.Sigmoid(),\n",
        ")\n",
        "\n",
        "CNNPoor.to(device)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): ZeroPad2d(padding=(63, 64, 0, 0), value=0.0)\n",
              "  (1): Conv2d(1, 4, kernel_size=(1, 128), stride=(1, 1), bias=False)\n",
              "  (2): ELU(alpha=1.0)\n",
              "  (3): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (4): Conv2d(4, 8, kernel_size=(64, 1), stride=(1, 1), groups=4, bias=False)\n",
              "  (5): ELU(alpha=1.0)\n",
              "  (6): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (7): AvgPool2d(kernel_size=(1, 4), stride=(1, 4), padding=0)\n",
              "  (8): Dropout(p=0.5, inplace=False)\n",
              "  (9): Conv2d(8, 16, kernel_size=(1, 32), stride=(1, 1), groups=8, bias=False)\n",
              "  (10): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "  (11): ELU(alpha=1.0)\n",
              "  (12): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (13): AvgPool2d(kernel_size=(1, 8), stride=(1, 8), padding=0)\n",
              "  (14): Dropout(p=0.5, inplace=False)\n",
              "  (15): Flatten()\n",
              "  (16): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRsayVFf_66T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_function = nn.BCELoss()\n",
        "optimizer = optim.Adam(CNNPoor.parameters(), lr = 0.001)"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFBd1M29CFmp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model, device, test_loader):\n",
        "    \n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    tot_loss = 0\n",
        "    acc_score, prec_score, rec_score = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for (data, labels) in test_loader:\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "            \n",
        "            classification = model(data.float())\n",
        "            loss = loss_function(classification, labels)\n",
        "\n",
        "            pred = torch.round(classification)\n",
        "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "            tot_loss += loss.item()\n",
        "\n",
        "            acc_score += accuracy_score(labels, pred)\n",
        "            prec_score += precision_score(labels, pred)\n",
        "            rec_score += recall_score(labels, pred)\n",
        "\n",
        "        print(\"\\nTest set: Average loss: {:.6f}, Accuracy: {:.6f}\".format(tot_loss / test_batches, \n",
        "                                                                          correct / len(test_loader.dataset)))\n",
        "        print(\"sklearn accuracy: {:.6f} precision: {:.6f} recall: {:.6f}\\n\".format(acc_score / test_batches,\n",
        "                                                                                   prec_score / test_batches,\n",
        "                                                                                   rec_score / test_batches))"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fm36EBCCJ1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "\n",
        "    model.train()\n",
        "    correct = 0\n",
        "    batch = 0\n",
        "    tot_loss = 0\n",
        "    for (data, labels) in train_loader:\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        classification = model(data.float())\n",
        "        loss = loss_function(classification, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        pred = torch.round(classification)\n",
        "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "        tot_loss += loss.item()\n",
        "\n",
        "        batch += 1\n",
        "\n",
        "        if batch == train_batches:\n",
        "            print(\"Epoch: {}\".format(epoch))\n",
        "            print(\"\\tAverage loss: {:.6f}\".format(tot_loss / batch))\n",
        "            print(\"\\tAccuracy: {:.6f}\".format(correct / len(train_loader.dataset)))"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxLzxLHdCYKM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "outputId": "05d8d2e4-7ec9-4c5b-e577-1282aec2736f"
      },
      "source": [
        "for epoch in range(25):\n",
        "    train(CNNPoor, device, train_set_loader, optimizer, epoch)\n",
        "    test(CNNPoor, device, test_set_loader)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8, 8])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-95-7527ef811a81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCNNPoor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCNNPoor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-94-db611dfdb178>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mclassification\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2475\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2476\u001b[0m         raise ValueError(\"Target and input must have the same number of elements. target nelement ({}) \"\n\u001b[0;32m-> 2477\u001b[0;31m                          \"!= input nelement ({})\".format(target.numel(), input.numel()))\n\u001b[0m\u001b[1;32m   2478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2479\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Target and input must have the same number of elements. target nelement (8) != input nelement (64)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZSSbDW4hQwZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}