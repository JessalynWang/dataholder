{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EEGNet_current_full_copy_labeled.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D415xY3P1PCN",
        "colab_type": "text"
      },
      "source": [
        "# Loose EEGNet\n",
        "This notebook is a neural network that is based as much off of the EEGNet paper as I could understand."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sdi7BvnG2eiJ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VIVsTD-24Dj",
        "colab_type": "text"
      },
      "source": [
        "# Import/Install all necessary packages and libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKZ_PXcuvZIx",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "31829c8b-ec2b-4eaa-8306-93704ad33c89"
      },
      "source": [
        "!pip install mne"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mne in /usr/local/lib/python3.6/dist-packages (0.20.8)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from mne) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from mne) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anVJZVW0sUwY",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, precision_score, recall_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable, gradcheck\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from matplotlib import pyplot\n",
        "\n",
        "import mne\n",
        "from mne.io import concatenate_raws, read_raw_fif\n",
        "import mne.viz\n",
        "\n",
        "import math\n",
        "\n",
        "from os import walk"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4ou4gyC4dQu",
        "colab_type": "text"
      },
      "source": [
        "# Check for GPU availability and set device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_rTCb13APF7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6t-hO2E4mHe",
        "colab_type": "text"
      },
      "source": [
        "# Load and format the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PrZ39Hzvbcu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "b9f7950a-375f-4714-e89a-ab007e7c7f72"
      },
      "source": [
        "data_file = '/content/drive/My Drive/data/P_01.fif'\n",
        "\n",
        "epochs = mne.read_epochs(data_file, verbose='error')\n",
        "print(epochs.info)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<Info | 10 non-empty values\n",
            " bads: []\n",
            " ch_names: Fp1, AF7, AF3, F1, F3, F5, F7, FT7, FC5, FC3, FC1, C1, C3, C5, ...\n",
            " chs: 64 EEG\n",
            " custom_ref_applied: False\n",
            " dig: 67 items (3 Cardinal, 64 EEG)\n",
            " file_id: 4 items (dict)\n",
            " highpass: 0.0 Hz\n",
            " lowpass: 128.0 Hz\n",
            " meas_date: unspecified\n",
            " meas_id: 4 items (dict)\n",
            " nchan: 64\n",
            " projs: []\n",
            " sfreq: 256.0 Hz\n",
            ">\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FDYPc5GvwTv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "0690fe64-c642-4a75-d6be-d8fba6f8565a"
      },
      "source": [
        "epochs_UN = epochs['FU', 'FN'] # Unpleasant vs. Neutral\n",
        "epochs_UP = epochs['FU', 'FP'] # Unpleasant vs. Pleasant\n",
        "epochs_NP = epochs['FN', 'FP'] # Neutral vs. Pleasant\n",
        "\n",
        "# Dataset with unpleasant and neutral events\n",
        "print(epochs_UN)\n",
        "data_UN = epochs_UN.get_data() #we will classify between unpleasant and neutral\n",
        "labels_UN = epochs_UN.events[:,-1]\n",
        "print(len(labels_UN))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<EpochsFIF  |   249 events (all good), 0 - 1.49609 sec, baseline off, ~46.9 MB, data loaded,\n",
            " 'FN': 126\n",
            " 'FU': 123>\n",
            "249\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PzcJwNMv4QU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.manual_seed(3)\n",
        "torch.cuda.manual_seed(3)\n",
        "train_data_UN, test_data_UN, labels_train_UN, labels_test_UN = train_test_split(data_UN, labels_UN, test_size=0.3, random_state=42)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnIbNDuBv917",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "92e580f1-573b-4ba6-af44-29b0bf24b793"
      },
      "source": [
        "print(labels_train_UN.shape, labels_test_UN.shape, train_data_UN.shape[-1])\n",
        "chunk_train = labels_train_UN.shape[0]\n",
        "chunk_test = labels_test_UN.shape[0]\n",
        "channels = train_data_UN.shape[1]\n",
        "timepoints = train_data_UN.shape[2]\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(174,) (75,) 384\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f6NOpVZwDwQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "8116804c-47ed-4ee0-cc4b-a59befbcc77f"
      },
      "source": [
        "BATCH_SIZE = 8\n",
        "BATCH_SIZE2 = chunk_test\n",
        "\n",
        "eeg_data_scaler = StandardScaler()\n",
        "\n",
        "X_train = eeg_data_scaler.fit_transform(train_data_UN.reshape(-1, train_data_UN.shape[-1])).reshape(train_data_UN.shape)\n",
        "X_test = eeg_data_scaler.fit_transform(test_data_UN.reshape(-1, test_data_UN.shape[-1])).reshape(test_data_UN.shape)\n",
        "\n",
        "labels_train_UN = np.array([1 if x > 0 else 0 for x in labels_train_UN])\n",
        "labels_test_UN = np.array([1 if x > 0 else 0 for x in labels_test_UN])\n",
        "\n",
        "labels_train_UN = labels_train_UN.reshape((chunk_train, 1))\n",
        "labels_train_UN = labels_train_UN.astype(np.float32)\n",
        "X_actual = torch.from_numpy(labels_train_UN)\n",
        "\n",
        "labels_test_UN = labels_test_UN.reshape((chunk_test, 1))\n",
        "labels_test_UN = labels_test_UN.astype(np.float32)\n",
        "X_test_actual = torch.from_numpy(labels_test_UN)\n",
        "\n",
        "X_train = torch.from_numpy(X_train)\n",
        "X_train = X_train.unsqueeze(1)\n",
        "X_test = torch.from_numpy(X_test)\n",
        "X_test = X_test.unsqueeze(1)\n",
        "\n",
        "train_batches = math.ceil(chunk_train / BATCH_SIZE)\n",
        "test_batches = math.ceil(chunk_test / BATCH_SIZE2)\n",
        "print(X_train.shape, X_actual.shape, X_test.shape, train_batches, test_batches)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([174, 1, 64, 384]) torch.Size([174, 1]) torch.Size([75, 1, 64, 384]) 22 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vz3MXsZebdIE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set = TensorDataset(X_train, X_actual)\n",
        "test_set = TensorDataset(X_test, X_test_actual)\n",
        "\n",
        "train_set_loader = DataLoader(train_set, batch_size = BATCH_SIZE, shuffle=False)\n",
        "test_set_loader = DataLoader(test_set, batch_size = BATCH_SIZE2, shuffle=False)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCqJTTUu4xuf",
        "colab_type": "text"
      },
      "source": [
        "# Build the model and train\n",
        "A short breakdown of the paper:\n",
        "> EEGNet is a CNN for classification and interpretation of EEG-based BCI's\n",
        "\n",
        "> Benefits of EEGNet:\n",
        "> 1. Can be applied to different BCI paradagims (MI, ERP, SSVEP)\n",
        "> 2. Can be trained with very limited data\n",
        "> 3. Can produce neurophysiologically interpretable features\n",
        "\n",
        "> The model architecture:\n",
        "> 1. Fit 2D convolutional filters of size (1, sampling rate//2)\n",
        "> 2. Use a depthwise convolution of size (# of channels, 1)\n",
        "> 3. Add a separable convolution of size (1, 16)\n",
        "> 4. Flatten the data and feed it through a classification layer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQpyCFRxx24X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hyperparameters\n",
        "freq, avg1stride, avg2stride = 256, (1, 4), (1, 8)\n",
        "convstride = 1 # stride for each conv2D\n",
        "conv1_neurons = 2\n",
        "conv2_neurons = 4\n",
        "conv3_neurons = 8\n",
        "conv4_neurons = 4\n",
        "kern1size = freq // 2\n",
        "kern3size = 32"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpvKD47DQ8FS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "padding_needed = (kern1size - 1) / 2\n",
        "conv1outx, conv1outy = (channels, (timepoints + (2 * padding_needed) - kern1size)/convstride + 1)\n",
        "\n",
        "conv2outx, conv2outy = ((conv1outx - channels)/convstride + 1, conv1outy)\n",
        "conv2outx, conv2outy = conv2outx // avg1stride[0], conv2outy // avg1stride[1]\n",
        "\n",
        "conv3outx, conv3outy = (conv2outx, (conv2outy - kern3size)/convstride + 1)\n",
        "\n",
        "conv4outx, conv4outy = (conv3outx, conv3outy)\n",
        "conv4outx, conv4outy = (conv4outx // avg2stride[0], conv4outy // avg2stride[1])\n",
        "flat1_in = int(conv4outx * conv4outy * conv4_neurons)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgHA3ZVMfyhn",
        "colab_type": "text"
      },
      "source": [
        "Model description, layer by layer\n",
        "\n",
        "1. Temporal convolution\n",
        "> * - Uses 4 filters of (1, sampling frequency // 2)\n",
        "> * - Learns frequency filters at 2Hz and above\n",
        "2. Depthwise convolution\n",
        "> * - Learns spatial filters\n",
        "3. Separable Convolution\n",
        "> * - Conists of a deptwise convolution followed by a pointwise convolution\n",
        "> * - First learnes a kernel summarizing each feature map, then merges the outputs\n",
        "4. Fully connected layer\n",
        "> * - Consists of a linear layer that reduces the channels, followed by a sigmoid classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCNgBinI_3Oq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "37c6bdd3-4f33-46db-fa30-668e1f9728f0"
      },
      "source": [
        "CNNPoor = nn.Sequential(\n",
        "    nn.ZeroPad2d((math.floor(padding_needed), math.ceil(padding_needed), 0, 0)),\n",
        "    nn.Conv2d(1, conv1_neurons, (1, kern1size), bias=False),\n",
        "    nn.ELU(),\n",
        "    nn.BatchNorm2d(conv1_neurons),\n",
        "    \n",
        "    nn.Conv2d(conv1_neurons, conv2_neurons, (channels, 1), bias=False, groups = conv1_neurons),\n",
        "    nn.ELU(),\n",
        "    nn.BatchNorm2d(conv2_neurons),\n",
        "    nn.AvgPool2d(avg1stride),\n",
        "    nn.Dropout(),\n",
        "    \n",
        "    nn.Conv2d(conv2_neurons, conv3_neurons, (1, kern3size), bias=False, groups = conv2_neurons),\n",
        "    nn.Conv2d(conv3_neurons, conv4_neurons, kernel_size=1, bias=False),\n",
        "    nn.ELU(),\n",
        "    nn.BatchNorm2d(conv4_neurons),\n",
        "    nn.AvgPool2d(avg2stride),\n",
        "    nn.Dropout(),\n",
        "    \n",
        "    nn.Flatten(),\n",
        "\n",
        "    nn.Linear(flat1_in, 1),\n",
        "    nn.Sigmoid(),\n",
        ")\n",
        "\n",
        "CNNPoor.to(device)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): ZeroPad2d(padding=(63, 64, 0, 0), value=0.0)\n",
              "  (1): Conv2d(1, 2, kernel_size=(1, 128), stride=(1, 1), bias=False)\n",
              "  (2): ELU(alpha=1.0)\n",
              "  (3): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (4): Conv2d(2, 4, kernel_size=(64, 1), stride=(1, 1), groups=2, bias=False)\n",
              "  (5): ELU(alpha=1.0)\n",
              "  (6): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (7): AvgPool2d(kernel_size=(1, 4), stride=(1, 4), padding=0)\n",
              "  (8): Dropout(p=0.5, inplace=False)\n",
              "  (9): Conv2d(4, 8, kernel_size=(1, 32), stride=(1, 1), groups=4, bias=False)\n",
              "  (10): Conv2d(8, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "  (11): ELU(alpha=1.0)\n",
              "  (12): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (13): AvgPool2d(kernel_size=(1, 8), stride=(1, 8), padding=0)\n",
              "  (14): Dropout(p=0.5, inplace=False)\n",
              "  (15): Flatten()\n",
              "  (16): Linear(in_features=32, out_features=1, bias=True)\n",
              "  (17): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRsayVFf_66T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_function = nn.BCELoss()\n",
        "optimizer = optim.Adam(CNNPoor.parameters(), lr = 0.001)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFBd1M29CFmp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model, device, test_loader):\n",
        "    \n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    tot_loss = 0\n",
        "    acc_score, prec_score, rec_score = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for (data, labels) in test_loader:\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "            \n",
        "            classification = model(data.float())\n",
        "            loss = loss_function(classification, labels)\n",
        "\n",
        "            pred = torch.round(classification)\n",
        "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "            tot_loss += loss.item()\n",
        "\n",
        "            acc_score += accuracy_score(labels, pred)\n",
        "            prec_score += precision_score(labels, pred)\n",
        "            rec_score += recall_score(labels, pred)\n",
        "\n",
        "        print(\"\\nTest set: Average loss: {:.6f}, Accuracy: {:.6f}\".format(tot_loss / test_batches, \n",
        "                                                                          correct / len(test_loader.dataset)))\n",
        "        print(\"sklearn accuracy: {:.6f} precision: {:.6f} recall: {:.6f}\\n\".format(acc_score / test_batches,\n",
        "                                                                                   prec_score / test_batches,\n",
        "                                                                                   rec_score / test_batches))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fm36EBCCJ1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "\n",
        "    model.train()\n",
        "    correct = 0\n",
        "    batch = 0\n",
        "    tot_loss = 0\n",
        "    for (data, labels) in train_loader:\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        classification = model(data.float())\n",
        "        loss = loss_function(classification, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        pred = torch.round(classification)\n",
        "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "        tot_loss += loss.item()\n",
        "\n",
        "        batch += 1\n",
        "\n",
        "        if batch == train_batches:\n",
        "            print(\"Epoch: {}\".format(epoch))\n",
        "            print(\"\\tAverage loss: {:.6f}\".format(tot_loss / batch))\n",
        "            print(\"\\tAccuracy: {:.6f}\".format(correct / len(train_loader.dataset)))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxLzxLHdCYKM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "241b00e1-b7c2-4f1d-b542-7e9488328fde"
      },
      "source": [
        "for epoch in range(25):\n",
        "    train(CNNPoor, device, train_set_loader, optimizer, epoch)\n",
        "    test(CNNPoor, device, test_set_loader)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "\tAverage loss: 0.731682\n",
            "\tAccuracy: 0.465517\n",
            "\n",
            "Test set: Average loss: 0.687760, Accuracy: 0.546667\n",
            "sklearn accuracy: 0.546667 precision: 0.608696 recall: 0.358974\n",
            "\n",
            "Epoch: 1\n",
            "\tAverage loss: 0.705173\n",
            "\tAccuracy: 0.511494\n",
            "\n",
            "Test set: Average loss: 0.674062, Accuracy: 0.600000\n",
            "sklearn accuracy: 0.600000 precision: 0.621622 recall: 0.589744\n",
            "\n",
            "Epoch: 2\n",
            "\tAverage loss: 0.687716\n",
            "\tAccuracy: 0.568966\n",
            "\n",
            "Test set: Average loss: 0.662085, Accuracy: 0.600000\n",
            "sklearn accuracy: 0.600000 precision: 0.604651 recall: 0.666667\n",
            "\n",
            "Epoch: 3\n",
            "\tAverage loss: 0.679021\n",
            "\tAccuracy: 0.540230\n",
            "\n",
            "Test set: Average loss: 0.649008, Accuracy: 0.680000\n",
            "sklearn accuracy: 0.680000 precision: 0.659574 recall: 0.794872\n",
            "\n",
            "Epoch: 4\n",
            "\tAverage loss: 0.671446\n",
            "\tAccuracy: 0.557471\n",
            "\n",
            "Test set: Average loss: 0.646912, Accuracy: 0.666667\n",
            "sklearn accuracy: 0.666667 precision: 0.684211 recall: 0.666667\n",
            "\n",
            "Epoch: 5\n",
            "\tAverage loss: 0.634561\n",
            "\tAccuracy: 0.678161\n",
            "\n",
            "Test set: Average loss: 0.631753, Accuracy: 0.733333\n",
            "sklearn accuracy: 0.733333 precision: 0.702128 recall: 0.846154\n",
            "\n",
            "Epoch: 6\n",
            "\tAverage loss: 0.632044\n",
            "\tAccuracy: 0.666667\n",
            "\n",
            "Test set: Average loss: 0.620172, Accuracy: 0.720000\n",
            "sklearn accuracy: 0.720000 precision: 0.714286 recall: 0.769231\n",
            "\n",
            "Epoch: 7\n",
            "\tAverage loss: 0.629063\n",
            "\tAccuracy: 0.672414\n",
            "\n",
            "Test set: Average loss: 0.603007, Accuracy: 0.760000\n",
            "sklearn accuracy: 0.760000 precision: 0.744186 recall: 0.820513\n",
            "\n",
            "Epoch: 8\n",
            "\tAverage loss: 0.632957\n",
            "\tAccuracy: 0.632184\n",
            "\n",
            "Test set: Average loss: 0.600747, Accuracy: 0.720000\n",
            "sklearn accuracy: 0.720000 precision: 0.781250 recall: 0.641026\n",
            "\n",
            "Epoch: 9\n",
            "\tAverage loss: 0.658640\n",
            "\tAccuracy: 0.614943\n",
            "\n",
            "Test set: Average loss: 0.584028, Accuracy: 0.746667\n",
            "sklearn accuracy: 0.746667 precision: 0.763158 recall: 0.743590\n",
            "\n",
            "Epoch: 10\n",
            "\tAverage loss: 0.601102\n",
            "\tAccuracy: 0.695402\n",
            "\n",
            "Test set: Average loss: 0.561603, Accuracy: 0.773333\n",
            "sklearn accuracy: 0.773333 precision: 0.761905 recall: 0.820513\n",
            "\n",
            "Epoch: 11\n",
            "\tAverage loss: 0.585418\n",
            "\tAccuracy: 0.701149\n",
            "\n",
            "Test set: Average loss: 0.549630, Accuracy: 0.720000\n",
            "sklearn accuracy: 0.720000 precision: 0.725000 recall: 0.743590\n",
            "\n",
            "Epoch: 12\n",
            "\tAverage loss: 0.559282\n",
            "\tAccuracy: 0.706897\n",
            "\n",
            "Test set: Average loss: 0.518583, Accuracy: 0.760000\n",
            "sklearn accuracy: 0.760000 precision: 0.783784 recall: 0.743590\n",
            "\n",
            "Epoch: 13\n",
            "\tAverage loss: 0.573557\n",
            "\tAccuracy: 0.695402\n",
            "\n",
            "Test set: Average loss: 0.522529, Accuracy: 0.800000\n",
            "sklearn accuracy: 0.800000 precision: 0.800000 recall: 0.820513\n",
            "\n",
            "Epoch: 14\n",
            "\tAverage loss: 0.523107\n",
            "\tAccuracy: 0.724138\n",
            "\n",
            "Test set: Average loss: 0.521694, Accuracy: 0.786667\n",
            "sklearn accuracy: 0.786667 precision: 0.780488 recall: 0.820513\n",
            "\n",
            "Epoch: 15\n",
            "\tAverage loss: 0.499528\n",
            "\tAccuracy: 0.775862\n",
            "\n",
            "Test set: Average loss: 0.500087, Accuracy: 0.760000\n",
            "sklearn accuracy: 0.760000 precision: 0.744186 recall: 0.820513\n",
            "\n",
            "Epoch: 16\n",
            "\tAverage loss: 0.497046\n",
            "\tAccuracy: 0.764368\n",
            "\n",
            "Test set: Average loss: 0.493313, Accuracy: 0.760000\n",
            "sklearn accuracy: 0.760000 precision: 0.783784 recall: 0.743590\n",
            "\n",
            "Epoch: 17\n",
            "\tAverage loss: 0.471277\n",
            "\tAccuracy: 0.816092\n",
            "\n",
            "Test set: Average loss: 0.479736, Accuracy: 0.773333\n",
            "sklearn accuracy: 0.773333 precision: 0.750000 recall: 0.846154\n",
            "\n",
            "Epoch: 18\n",
            "\tAverage loss: 0.460344\n",
            "\tAccuracy: 0.804598\n",
            "\n",
            "Test set: Average loss: 0.480577, Accuracy: 0.773333\n",
            "sklearn accuracy: 0.773333 precision: 0.789474 recall: 0.769231\n",
            "\n",
            "Epoch: 19\n",
            "\tAverage loss: 0.466778\n",
            "\tAccuracy: 0.787356\n",
            "\n",
            "Test set: Average loss: 0.444503, Accuracy: 0.786667\n",
            "sklearn accuracy: 0.786667 precision: 0.794872 recall: 0.794872\n",
            "\n",
            "Epoch: 20\n",
            "\tAverage loss: 0.446030\n",
            "\tAccuracy: 0.821839\n",
            "\n",
            "Test set: Average loss: 0.455548, Accuracy: 0.840000\n",
            "sklearn accuracy: 0.840000 precision: 0.813953 recall: 0.897436\n",
            "\n",
            "Epoch: 21\n",
            "\tAverage loss: 0.421663\n",
            "\tAccuracy: 0.867816\n",
            "\n",
            "Test set: Average loss: 0.446557, Accuracy: 0.800000\n",
            "sklearn accuracy: 0.800000 precision: 0.815789 recall: 0.794872\n",
            "\n",
            "Epoch: 22\n",
            "\tAverage loss: 0.378164\n",
            "\tAccuracy: 0.862069\n",
            "\n",
            "Test set: Average loss: 0.457098, Accuracy: 0.800000\n",
            "sklearn accuracy: 0.800000 precision: 0.815789 recall: 0.794872\n",
            "\n",
            "Epoch: 23\n",
            "\tAverage loss: 0.413214\n",
            "\tAccuracy: 0.839080\n",
            "\n",
            "Test set: Average loss: 0.423404, Accuracy: 0.813333\n",
            "sklearn accuracy: 0.813333 precision: 0.790698 recall: 0.871795\n",
            "\n",
            "Epoch: 24\n",
            "\tAverage loss: 0.354104\n",
            "\tAccuracy: 0.850575\n",
            "\n",
            "Test set: Average loss: 0.415722, Accuracy: 0.853333\n",
            "sklearn accuracy: 0.853333 precision: 0.850000 recall: 0.871795\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZSSbDW4hQwZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": []
    }
  ]
}